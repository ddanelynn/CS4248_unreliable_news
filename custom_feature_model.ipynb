{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Project - Labelled Unreliable News (LUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from readability import Readability\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = None\n",
    "# lemmatizer = None\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "SMOOTHING = 1.0\n",
    "NGRAM_RANGE = (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, lower_case=True, remove_punctuation=True, replace_contractions=True):\n",
    "    if lower_case:\n",
    "        sentence = sentence.lower()\n",
    "    if remove_punctuation:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    if replace_contractions:\n",
    "        news = sentence.split()\n",
    "        new_news = []\n",
    "        for word in news:\n",
    "            if word in contractions:\n",
    "                new_news.append(contractions[word])\n",
    "            else:\n",
    "                new_news.append(word)\n",
    "        sentence = \" \".join(new_news)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, stemmer=stemmer, lemmatizer=lemmatizer, remove_stop_words=False):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if remove_stop_words:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    if lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv('raw_data/fulltrain.csv', header=None)\n",
    "full_train_df.columns = ['label', 'text']\n",
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = full_train_df.drop_duplicates(subset=['text'])\n",
    "print(f\"No. training samples (all classes): {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction_count(text):\n",
    "    count = 0\n",
    "    for contract in contractions:\n",
    "        count += re.subn(contract, '', text)[1]\n",
    "    return count\n",
    "\n",
    "def count_stopwords(text):  \n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_list = [w for w in word_tokens if w in stopwords]\n",
    "    return len(stopwords_list)\n",
    "\n",
    "def count_special_characters(text):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        if(not text[i].isalpha() and not text[i].isdigit()):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_uppercase(text):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        if(text[i].isupper()):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_lowercase(text):\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        if(text[i].islower()):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_noun_verb_adverb_adjective(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_tagged = pos_tag(word_tokens, tagset='universal')\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    adj_count = 0\n",
    "    for pair in word_tagged:\n",
    "        tag = pair[1]\n",
    "        if tag == 'NOUN':\n",
    "            noun_count += 1\n",
    "        elif tag == 'VERB':\n",
    "            verb_count += 1\n",
    "        elif tag == 'ADV':\n",
    "            adv_count += 1\n",
    "        elif tag == 'ADJ':\n",
    "            adj_count += 1\n",
    "    return (noun_count, verb_count, adv_count, adj_count)\n",
    "\n",
    "def count_syllable(text):\n",
    "    words = word_tokenize(text)\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    for word in words:\n",
    "        count += count_syllable_in_word(word, vowels)\n",
    "    return count\n",
    "\n",
    "def count_syllable_in_word(word, vowels):\n",
    "    count = 0\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def get_gunning_fog_grade_index(text):\n",
    "    # 0.4 [(words/sentences) + 100 (complex words/words)]\n",
    "    # Complex words are those containing three or more syllables.\n",
    "    sents = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    num_sents = len(sents)\n",
    "    num_words = len(words)\n",
    "    num_complex_words = 0\n",
    "    for word in words:\n",
    "        if count_syllable_in_word(word, \"aeiouy\") >= 3:\n",
    "            num_complex_words += 1\n",
    "    return 0.4*(num_words/num_sents) + 100*(num_complex_words/num_words)\n",
    "    \n",
    "def get_dale_chall_readability_coleman_liau_index(text):\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        return (r.dale_chall(), r.coleman_liau())\n",
    "    except:\n",
    "        return (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity Features\n",
    "train_df['num_stopwords'] = train_df['text'].apply(lambda x: count_stopwords(x))\n",
    "train_df['num_sentences'] = train_df['text'].apply(lambda x: len(str(x).split('.')))\n",
    "train_df['num_contractions'] = train_df['text'].apply(lambda x: contraction_count(x))\n",
    "train_df['num_special_characters'] = train_df['text'].apply(lambda x: count_special_characters(x))\n",
    "train_df['num_uppercase'] = train_df['text'].apply(lambda x: count_uppercase(x))\n",
    "train_df['num_lowercase'] = train_df['text'].apply(lambda x: count_lowercase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylometric Features\n",
    "train_df['pos_tags'] = train_df['text'].apply(lambda x: count_noun_verb_adverb_adjective(x))\n",
    "train_df['num_noun'] = train_df['pos_tags'].apply(lambda x: x[0])\n",
    "train_df['num_verb'] = train_df['pos_tags'].apply(lambda x: x[1])\n",
    "train_df['num_adverb'] = train_df['pos_tags'].apply(lambda x: x[2])\n",
    "train_df['num_adjective'] = train_df['pos_tags'].apply(lambda x: x[3])\n",
    "train_df['num_syllables'] = train_df['text'].apply(lambda x: count_syllable(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability based evidence\n",
    "train_df['gunning_fog_grade_index'] = train_df['text'].apply(lambda x: get_gunning_fog_grade_index(x))\n",
    "train_df['readability_indices'] = train_df['text'].apply(lambda x: get_dale_chall_readability_coleman_liau_index(x))\n",
    "train_df['dale_chall_readability'] = train_df['readability_indices'].apply(lambda x: x[0])\n",
    "train_df['coleman_liau_index'] = train_df['readability_indices'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psycho-linguistic features\n",
    "train_df['psycho-linguistic'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment)\n",
    "train_df['polarity'] = train_df['psycho-linguistic'].apply(lambda x: x[0])\n",
    "train_df['subjectivity'] = train_df['psycho-linguistic'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE, smooth_idf=True, preprocessor=preprocess, tokenizer=tokenize, token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['num_stopwords', 'num_sentences', 'num_contractions', 'num_special_characters',\n",
    "            'num_uppercase', 'num_lowercase',\n",
    "            'num_noun', 'num_verb', 'num_adverb', 'num_adjective', 'num_syllables',\n",
    "            'gunning_fog_grade_index', 'dale_chall_readability', 'coleman_liau_index',\n",
    "            'polarity', 'subjectivity', ]\n",
    "y = train_df['label'].values\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'].values).toarray()\n",
    "train_tfidf = pd.DataFrame(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = pd.merge(train_tfidf, train_df[features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=TEST_SIZE, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=200).fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_train_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_train, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('raw_data/balancedtest.csv', header=None, names=['label', 'text'])\n",
    "print(f\"No. test samples (all classes): {len(test_df)}\")\n",
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = test_df['text'].values\n",
    "y_val = test_df['label'].values\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_val_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = clf.predict(X_val_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_val)}\")\n",
    "print(f\"F1 score: {f1_score(y_val, y_pred_val, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred_val, target_names=['satire', 'hoax', 'propaganda', 'reliable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
