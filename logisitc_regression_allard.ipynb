{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Project - Labelled Unreliable News (LUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/allard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = None\n",
    "# lemmatizer = None\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "SMOOTHING = 1.0\n",
    "NGRAM_RANGE = (1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, lower_case=True, remove_punctuation=True):\n",
    "    if lower_case:\n",
    "        sentence = sentence.lower()\n",
    "    if remove_punctuation:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, stemmer=stemmer, lemmatizer=lemmatizer, remove_stop_words=False):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    if remove_stop_words:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    if lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_train_df = pd.read_csv('raw_data/fulltrain.csv', header=None)\n",
    "full_train_df.columns = ['label', 'text']\n",
    "print(len(full_train_df))\n",
    "full_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. training samples (all classes): 48652\n",
      "No. training samples (classes 1 and 2): 20850\n"
     ]
    }
   ],
   "source": [
    "train_df = full_train_df.drop_duplicates(subset=['text'])\n",
    "subset_df = train_df[train_df['label'].isin([1, 2])]\n",
    "print(f\"No. training samples (all classes): {len(train_df)}\")\n",
    "print(f\"No. training samples (classes 1 and 2): {len(subset_df)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18765,), (2085,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = subset_df['text'].values\n",
    "y = subset_df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE, smooth_idf=True, preprocessor=preprocess, tokenizer=tokenize, token_pattern=None)\n",
    "# Uncomment for default TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(ngram_range=NGRAM_RANGE, smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 107168\n",
      "['0' '00' '000' '00000000' '00000000001' '0000000001ounce' '000000003'\n",
      " '00000001' '0000001' '0000004' '000002' '000006' '000013s' '00003'\n",
      " '00004' '00005' '0001' '00010010101010101010101010101010' '00010052'\n",
      " '0003' '0004' '000567kln00067q' '001' '0010'\n",
      " '001000011011010101010101010101010101010010' '002' '003' '0035' '007'\n",
      " '0073735963' '00893' '009siam' '01' '010' '0100' '0100010' '0101010'\n",
      " '0110' '0115' '012' '014' '016' '017' '018' '01ers' '01oz' '02' '020'\n",
      " '021' '025' '03' '0302' '03130' '031mile' '03823' '038d' '03squaremile'\n",
      " '04' '042684425' '045' '047' '05' '0543' '0563' '0586' '05k' '06' '064'\n",
      " '07' '072' '075off' '078' '08' '085' '087centimeter' '08and' '08ounce'\n",
      " '08second' '09' '0900' '095400' '099' '0bama' '0bamas' '0for4' '0for5'\n",
      " '1' '10' '100' '1000' '10000' '100000' '1000000' '10000000000000000000'\n",
      " '100000aday' '10000acre' '10000aplate' '10000foot' '10000km' '10000m'\n",
      " '10000man' '10000name' '10000person' '10000squarefoot' '10000th'\n",
      " '10000year' '10000yearold' '1000aweekforlife' '1000bottle' '1000foottall'\n",
      " '1000i' '1000mile' '1000pound' '1000pump' '1000th' '1000ticket'\n",
      " '1000watt' '1001' '10020acre' '1003' '1003256' '1003inch' '10045' '1005'\n",
      " '100800' '100according' '100aplate' '100aportion' '100day' '100degree'\n",
      " '100foot' '100foottall' '100franc' '100hour' '100kilogram' '100kilometer'\n",
      " '100m' '100meter' '100mg' '100mile' '100milligram' '100minute' '100month'\n",
      " '100mphplus' '100percent' '100percentstoned' '100plus' '100point'\n",
      " '100round' '100seat']\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(feature_names[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18765, 107168)\n",
      "  (0, 67928)\t0.04690778335524813\n",
      "  (0, 21249)\t0.1333386348980611\n",
      "  (0, 43194)\t0.08310713344794932\n",
      "  (0, 32006)\t0.10953515566893744\n",
      "  (0, 8163)\t0.02869976456157917\n",
      "  (0, 98456)\t0.08821041617924698\n",
      "  (0, 104089)\t0.040846358021028074\n",
      "  (0, 34746)\t0.05842618131561038\n",
      "  (0, 68440)\t0.04632865702469982\n",
      "  (0, 88085)\t0.09197556341767808\n",
      "  (0, 27208)\t0.14193047969217695\n",
      "  (0, 5184)\t0.017927628630699844\n",
      "  (0, 104922)\t0.024296311553436957\n",
      "  (0, 56934)\t0.09661311129422885\n",
      "  (0, 87213)\t0.07548098878276675\n",
      "  (0, 69549)\t0.03238846408598244\n",
      "  (0, 76474)\t0.16859099801900168\n",
      "  (0, 102404)\t0.025875827153799776\n",
      "  (0, 96215)\t0.03193805789665119\n",
      "  (0, 75318)\t0.04892331870205253\n",
      "  (0, 78776)\t0.07397951229643021\n",
      "  (0, 7251)\t0.03172672066517962\n",
      "  (0, 95155)\t0.04390861784696202\n",
      "  (0, 86441)\t0.10020673063337579\n",
      "  (0, 43945)\t0.06339187505337111\n",
      "  :\t:\n",
      "  (18764, 105297)\t0.04112567606935367\n",
      "  (18764, 37608)\t0.05238050553398173\n",
      "  (18764, 104260)\t0.023713954774334255\n",
      "  (18764, 50475)\t0.09118795726712614\n",
      "  (18764, 47703)\t0.023398855571306065\n",
      "  (18764, 95083)\t0.0332041457777695\n",
      "  (18764, 54822)\t0.05608558778703029\n",
      "  (18764, 95549)\t0.020351749193070957\n",
      "  (18764, 8163)\t0.0456752063231171\n",
      "  (18764, 68440)\t0.0184328251520651\n",
      "  (18764, 5184)\t0.01426576400713129\n",
      "  (18764, 104922)\t0.03866718281653055\n",
      "  (18764, 69549)\t0.025772855669982483\n",
      "  (18764, 96215)\t0.050828897249634256\n",
      "  (18764, 102931)\t0.11182470754809896\n",
      "  (18764, 96521)\t0.08491732847541604\n",
      "  (18764, 38725)\t0.06763349363412352\n",
      "  (18764, 94966)\t0.09342549519905698\n",
      "  (18764, 82590)\t0.01995862227055121\n",
      "  (18764, 67915)\t0.08662792224427326\n",
      "  (18764, 8260)\t0.058687213844942986\n",
      "  (18764, 64387)\t0.03203369270795842\n",
      "  (18764, 44509)\t0.021489736603962438\n",
      "  (18764, 95035)\t0.19493511586301218\n",
      "  (18764, 48582)\t0.06117756749169579\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9897148947508659\n",
      "F1 score: 0.9883552691098243\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=200).fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_train_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_train, y_pred, average='macro')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9908872901678657\n",
      "F1 score: 0.9896919824361684\n"
     ]
    }
   ],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"F1 score: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label 1\n",
    "satire_sentence = \"If voting changed anything, they would make it illegal.\"\t\t\n",
    "\n",
    "# Label 2\n",
    "hoax_sentence = \"In a recent turn of events, Obama has declared that he will be joining the Republican Party, parterning with Donald Trump.\"\t\n",
    "\n",
    "X_val = [satire_sentence, hoax_sentence]\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "y_pred_val = clf.predict(X_test_tfidf)\n",
    "y_pred_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7e653ca629f27ebde81528cc52bfe051277467cb10f3ef3e48891bb1c90ba85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
